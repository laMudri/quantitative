In this section, we present a simply typed calculus \name{} that is parametric
in a set $\mathscr{R}$ of usage annotations, under conditions detailed in
\autoref{sec:annotations}.

\subsection{Types}

Our grammar of types and contexts is given in \autoref{fig:syntax}.
Throughout this paper, the colour \rescomment{green} highlights usage
annotations, which are generally the interesting things to watch in typing rules
and are the main point of departure between this system and a standard simply
typed $\lambda$-calculus.
As we show in \autoref{sec:dill}, these annotations allow us to subsume also
normal linear type systems. \fixme{and other coeffect systems}

\fixmeBA{mention that we really think of the usage annotations as ``refining'' the underlying types and terms}

\begin{figure}
  \begin{mathpar}
    \rescomment \rho, \rescomment \pi \in \mathscr R \and
    A, B, C \Coloneqq \base_k \mid \fun{A}{B} \mid \tensorOne \mid \tensor{A}{B}
    \mid \withTOne \mid \withT{A}{B} \mid \sumTZero \mid \sumT{A}{B} \mid
    \excl{\rho}{A} \mid \listT{A} \\
    \Gamma, \Delta \Coloneqq \cdot \mid \Gamma, x : A \and
    \resctx P, \resctx Q, \resctx R \Coloneqq
    \cdot \mid \resctx R, x^{\rescomment \rho} \and
    \ctx{\Gamma}{R} \Coloneqq
    \cdot \mid \ctx{\Gamma}{R}, \ctxvar{x}{A}{\rho}
  \end{mathpar}
  \caption{Basic syntax}
  \label{fig:syntax}
\end{figure}

We have a set of uninterpreted base types $\base_k$; functions $\fun{}{}$; two
kinds of finite products $\tensorOne$/$\tensor{}{}$ and
$\withTOne$/$\withT{}{}$; finite sums $\sumTZero$/$\sumT{}{}$; a graded modality
$\excl{\rho}{}$; and polymorphic lists.
These will be explained further when their typing rules are introduced in
\autoref{sec:rules}.

\fixmeBA{explain usage contexts on their own first?}
Our contexts are combinations of a typing context $\Gamma$ and a usage
context $\resctx R$, written $\ctx{\Gamma}{R}$.
It will sometimes be useful to talk only about one part at a time.
In particular, it will be useful to think of usage contexts as row
vectors, and to use standard linear algebra operations on them, when stating and
proving simultaneous substitution in \autoref{sec:admissible}.

\subsection{Usage annotations}
\label{sec:annotations}

\fixmeBA{In general, I don't like starting sentences with numbers or symbols...}

We assume that our set of annotations $\mathscr R$ forms a partially ordered
semiring $(\mathscr{R}, \subres, 0, +, 1, *)$.
These annotations adorn free variables.
$0$ represents non-usage, and $+$ represents the combination of separate usages.
We require $(\mathscr{R}, 0, +)$ to be a commutative monoid.
$1$ represents a single or plain usage, and $*$ applies a transformation upon
the type of usage allowed.
We require $(\mathscr{R}, 1, *)$ to be a monoid that is annihilated by and
distributes over the additive structure.
The order $\subres$ describes a sub-usaging relation.
We have $\pi \subres \rho$ if and only if $\pi$ is at most as permissive as
$\rho$.
This acts a lot like a subtyping relation; in particular, knowing
$\pi \subres \rho$ gives us a function of type
$\fun{\excl{\pi}{A}}{\excl{\rho}{A}}$.
We require $(\mathscr{R}, \subres)$ to be a partial order, and addition and
multiplication to be monotonic with respect to $\subres$.

In many cases of interest, $0$ and $+$ are respectively the top and meet of the
sub-usaging order, turning it into a bounded meet semilattice.
In these cases, with products ($\withT{A}{B}$) coincide with tensor products
($\tensor{A}{B}$), and usage inference is significantly simplified.
\fixme{rntz reference?} \fixmeBA{Don't mention inference here if we aren't going to do it in detail}

\fixmeBA{this should all have fwd references to where they are used.}

\begin{example}[Trivial]
  The one-element partially ordered semiring provides no usage restrictions.
  The calculus becomes equivalent to a simply typed $\lambda$-calculus.
\end{example}

\begin{example}[Linearity]
  \label{ex:annotations-linearity}
  We approximate counting usages.
  $0$ and $1$ represent exactly their respective number of usages.
  $\omega$ represents unrestricted usage.
  The effect of the bang ($\oc$) modality of linear logic is achieved by
  multiplication by $\omega$, which makes everything available become
  unrestricted.

  \begin{center}
    \begin{tabular}{>{$}c<{$}}
      0 \coloneqq 0 \\
      1 \coloneqq 1
    \end{tabular}%
    \hspace{0.5in}%
    \begin{tabular}{>{$}r<{$}|>{$}l<{$}>{$}l<{$}>{$}l<{$}}
      +      & 0      & 1      & \omega \\
      \hline
      0      & 0      & 1      & \omega \\
      1      & 1      & \omega & \omega \\
      \omega & \omega & \omega & \omega \\
    \end{tabular}%
    \hspace{0.5in}%
    \begin{tabular}{>{$}r<{$}|>{$}l<{$}>{$}l<{$}>{$}l<{$}}
      *      & 0      & 1      & \omega \\
      \hline
      0      & 0      & 0      & 0      \\
      1      & 0      & 1      & \omega \\
      \omega & 0      & \omega & \omega \\
    \end{tabular}%
    \hspace{0.5in}%
    \(
    \begin{tikzcd}[arrows=dash,row sep=0.5cm,column sep=0.5cm]
      0 \arrow[dr] && 1 \arrow[dl] \\
      & \omega &
    \end{tikzcd}
    \)
  \end{center}
\end{example}

\begin{example}[Precise usage counting]
  Usage annotations track exactly how many times a free variable is used.
  There is no annotation for unrestricted use, though an $\omega$ could be added
  analogously to how it is done in \exref{ex:annotations-linearity}, above all
  of the precise elements.

  \begin{center}
    \(\mathscr R \coloneqq \mathbb N\)
    \hspace{0.5in}%
    \begin{tabular}{>{$}c<{$}}
      0 \coloneqq 0 \\
      1 \coloneqq 1
    \end{tabular}%
    \hspace{0.5in}%
    \begin{tabular}{>{$}c<{$}}
      {+} \coloneqq {+} \\
      {*} \coloneqq {\times}
    \end{tabular}%
    \hspace{0.5in}%
    \(m \subres n \iff m = n\)
  \end{center}
\end{example}

\begin{example}[Monotonicity]
  Usage annotations track which ``way up'' we can use a free variable.
  % A variable can be used covariantly ($\uparrow$), contravariantly
  % ($\downarrow$), invariantly ($=$), or unrestrictedly ($?$).
  We require that the interpretation of an open term increases, with four
  possible conditions on the perturbations of the values plugged in to the free
  variables.
  We consider the valuation increasing ($\uparrow$), decreasing ($\downarrow$),
  changing arbitrarily ($0$), or staying the same ($=$).
  Note that the value always increases when the valuation stays the same ($=$),
  and the only way for the value to increase irrespective of the valuation ($0$)
  is for the value to be constant in the valuation (for example, the variable
  does not appear in the term at all).

  \begin{center}
    \begin{tabular}{>{$}c<{$}}
      0 \coloneqq {0} \\
      1 \coloneqq {\uparrow}
    \end{tabular}%
    \hspace{0.5in}%
    \begin{tabular}{>{$}r<{$}|>{$}l<{$}>{$}l<{$}>{$}l<{$}>{$}l<{$}}
      +          & 0          & \uparrow & \downarrow & \equiv \\
      \hline
      0          & 0          & \uparrow & \downarrow & \equiv \\
      \uparrow   & \uparrow   & \uparrow & \equiv     & \equiv \\
      \downarrow & \downarrow & \equiv   & \downarrow & \equiv \\
      \equiv     & \equiv     & \equiv   & \equiv     & \equiv \\
    \end{tabular}%
    \hspace{0.5in}%
    \begin{tabular}{>{$}r<{$}|>{$}l<{$}>{$}l<{$}>{$}l<{$}>{$}l<{$}}
      *          & 0 & \uparrow   & \downarrow & \equiv \\
      \hline
      0          & 0 & 0          & 0          & 0      \\
      \uparrow   & 0 & \uparrow   & \downarrow & \equiv \\
      \downarrow & 0 & \downarrow & \uparrow   & \equiv \\
      \equiv     & 0 & \equiv     & \equiv     & \equiv \\
    \end{tabular}%
    \hspace{0.5in}%
    \(
    \begin{tikzcd}[arrows=dash,row sep=0.5cm,column sep=0.5cm]
      & 0 & \\
      \uparrow \arrow[ur] && \downarrow \arrow[ul] \\
      & \equiv \arrow[ul]\arrow[ur] &
    \end{tikzcd}
    \)
  \end{center}

  Notice that $+$ is the meet of the sub-usaging order.
\end{example}

\begin{example}[Security levels]
  Similar to \cite{abadi99core}, our annotations can be security levels
  constraining where a variable can be seen.
  We assume a security lattice $\mathcal L$, and make the following choices.

  \begin{center}
    \(\mathscr R \coloneqq \mathcal L\)
    \hspace{0.5in}%
    \begin{tabular}{>{$}c<{$}}
      0 \coloneqq \top \\
      1 \coloneqq \bot
    \end{tabular}%
    \hspace{0.5in}%
    \begin{tabular}{>{$}c<{$}}
      {+} \coloneqq {\wedge} \\
      {*} \coloneqq {\vee}
    \end{tabular}%
    \hspace{0.5in}%
    \(x \subres y \iff x \leq y\)
  \end{center}
\end{example}

\begin{example}[Sensitivity analysis]
  Following \cite{reed10distance}, we can put an upper bound on the size of
  change observed in a term after a perturbation of a free variable by
  annotating with distances. 
  The distance tells us how much the output may vary by when the variable varies
  by a distance of $1$.

  \begin{center}
    \(\mathscr R \coloneqq \{x \in \mathbb R \mid x \geq 0\} \cup \{\infty\}\)
    \hspace{0.5in}%
    \begin{tabular}{>{$}c<{$}}
      0 \coloneqq 0 \\
      1 \coloneqq 1
    \end{tabular}%
    \hspace{0.5in}%
    \begin{tabular}{>{$}c<{$}}
      {+} \coloneqq {+} \\
      {*} \coloneqq {\times}
    \end{tabular}%
    \hspace{0.5in}%
    \(x \subres y \iff x \leq y\)
  \end{center}

  An annotation $0$ on a variable $x$ says that no matter how much $x$
  changes, the result will vary by $0$.
  Conversely, an annotation $\infty$ says that as soon as the variable changes
  by a positive amount, the result may vary arbitrarily.
\end{example}

\subsection{Typing \& provisioning}
\label{sec:rules}

\begin{figure}
  \begin{displaymath}
    \begin{array}{l}
      \textrm{Constants}~c \quad \textrm{Variables}~x \\
      \begin{array}{rrl}
        \textrm{Terms}~M, N & \Coloneqq & c \mid x \mid \lam{x}{A} \mid
                                          \app{M}{N} \mid \unit \mid
                                          \del{C}{M}{N} \mid \ten{M}{N} \\
                            & \mid & \prm{C}{M}{x}{y}{N} \mid \eat \mid
                                     \wth{M}{N} \mid \proj{i}{M} \mid
                                     \exf{C}{M} \\
                            & \mid & \inj{i}{M} \mid \cse{C}{M}{x}{N_0}{y}{N_1}
                                     \mid \bang{M} \\
                            & \mid & \bm{C}{M}{x}{N} \mid \nil \mid \cons{M}{N}
                                     \mid \fold{M}{N_n}{x}{y}{N_c}
      \end{array}
    \end{array}
  \end{displaymath}
  \caption{Term syntax}
  \label{fig:terms}
\end{figure}

\begin{figure}
  \begin{mathpar}
    \inferrule*[right=var]
    {(x : A) \in \Gamma
      \\ \subrctx{\resctx R}{\basis x}
    }
    {\ctx{\Gamma}{R} \vdash x : A}
    \and
    \inferrule*[right=const]
    {\subrctx{\resctx R}{\vct 0}}
    {\ctx{\Gamma}{R} \vdash c : A_c}
    \and
    \inferrule*[right=$\multimap$-I]
    {\ctx{\Gamma}{R}, \ctxvar{x}{A}{1} \vdash M : B}
    {\ctx{\Gamma}{R} \vdash \lam{x}{M} : \fun{A}{B}}
    \and
    \inferrule*[right=$\multimap$-E]
    {\ctx{\Gamma}{P} \vdash M : \fun{A}{B}
      \\ \ctx{\Gamma}{Q} \vdash N : A
      \\ \subrctx{\resctx R}{\resctx P + \resctx Q}
    }
    {\ctx{\Gamma}{R} \vdash \app{M}{N} : B}
    \and
    \inferrule*[right=$\otimes$-I]
    {\ctx{\Gamma}{P} \vdash M : A
      \\ \ctx{\Gamma}{Q} \vdash N : B
      \\ \subrctx{\resctx R}{\resctx P + \resctx Q}
    }
    {\ctx{\Gamma}{R} \vdash \ten{M}{N} : \tensor{A}{B}}
    \and
    \inferrule*[right=$\otimes$-E]
    {\ctx{\Gamma}{P} \vdash M : \tensor{A}{B}
      \\ \ctx{\Gamma}{Q}, \ctxvar{x}{A}{1}, \ctxvar{y}{B}{1} \vdash N : C
      \\ \subrctx{\resctx R}{\resctx P + \resctx Q}
    }
    {\ctx{\Gamma}{R} \vdash \prm{C}{M}{x}{y}{N} : C}
    \and
    \inferrule*[right=$\&$-I]
    {\ctx{\Gamma}{R} \vdash M : A
      \\ \ctx{\Gamma}{R} \vdash N : B
    }
    {\ctx{\Gamma}{R} \vdash \wth{M}{N} : \withT{A}{B}}
    \and
    \inferrule*[right=$\&$-E$_i$]
    {\ctx{\Gamma}{R} \vdash M : \withT{A_0}{A_1}}
    {\ctx{\Gamma}{R} \vdash \proj{i}{M} : A_i}
    \and
    \inferrule*[right=$1$-I]
    {\subrctx{\resctx R}{\vct 0}}
    {\ctx{\Gamma}{R} \vdash \unit : \tensorOne}
    \and
    \inferrule*[right=$1$-E]
    {\ctx{\Gamma}{P} \vdash M : \tensorOne
      \\ \ctx{\Gamma}{Q} \vdash N : A
      \\ \subrctx{\resctx R}{\resctx P + \resctx Q}
    }
    {\ctx{\Gamma}{R} \vdash \del{A}{M}{N} : A}
    \and
    \inferrule*[right=$\top$-I]
    { }
    {\ctx{\Gamma}{R} \vdash \eat : \withTOne}
    \and
    \text{(no $\top$-E)}
    \and
    \text{(no $0$-I)}
    \and
    \inferrule*[right=$0$-E$_i$]
    {\ctx{\Gamma}{P} \vdash M : \sumTZero
      \\ \subrctx{\resctx R}{\resctx P + \resctx Q}
    }
    {\ctx{\Gamma}{R} \vdash \exf{A}{M} : A}
    \and
    \inferrule*[right=$\oplus$-I]
    {\ctx{\Gamma}{R} \vdash M : A_i}
    {\ctx{\Gamma}{R} \vdash \inj{i}{M} : \sumT{A_0}{A_1}}
    \and
    \inferrule*[right=$\oplus$-E]
    {\ctx{\Gamma}{P} \vdash M : \sumT{A}{B}
      \\ \ctx{\Gamma}{Q}, \ctxvar{x}{A}{1} \vdash N_0 : C
      \\ \ctx{\Gamma}{Q}, \ctxvar{y}{B}{1} \vdash N_1 : C
      \\ \subrctx{\resctx R}{\resctx P + \resctx Q}
    }
    {\ctx{\Gamma}{R} \vdash \cse{C}{M}{x}{N_0}{y}{N_1} : C}
    \and
    \inferrule*[right=$\excl{\rho}{}$-I]
    {\ctx{\Gamma}{P} \vdash M : A
      \\ \subrctx{\resctx R}{\rescomment \rho * \resctx P}
    }
    {\ctx{\Gamma}{R} \vdash \bang{M} : \excl{\rho}{A}}
    \and
    \inferrule*[right=$\excl{\rho}{}$-E]
    {\ctx{\Gamma}{P} \vdash M : \excl{\rho}{A}
      \\ \ctx{\Gamma}{Q}, \ctxvar{x}{A}{\rho} \vdash N : B
      \\ \subrctx{\resctx R}{\resctx P + \resctx Q}
    }
    {\ctx{\Gamma}{R} \vdash \bm{B}{M}{x}{N} : B}
    \and
    \inferrule*[right=$\listT{}$-Inil]
    {\subrctx{\resctx R}{\vct 0}}
    {\ctx{\Gamma}{R} \vdash \nil : \listT A}
    \and
    \inferrule*[right=$\listT{}$-Icons]
    {\ctx{\Gamma}{P} \vdash M : A
      \\ \ctx{\Gamma}{Q} \vdash N : \listT A
      \\ \subrctx{\resctx R}{\resctx P + \resctx Q}
    }
    {\ctx{\Gamma}{R} \vdash \cons{M}{N} : \listT A}
    \and
    \inferrule*[right=$\listT{}$-E]
    {\ctx{\Gamma}{R} \vdash M : \listT A
      \\ \ctx{\Gamma}{\vct 0} \vdash N_n : C
      \\ \ctx{\Gamma}{\vct 0}, \ctxvar{h}{A}{1}, \ctxvar{r}{\listT A}{1} \vdash N_c : C
    }
    {\ctx{\Gamma}{R} \vdash \fold{M}{N_n}{h}{r}{N_c} : C}
  \end{mathpar}
  \caption{Typing rules for \name}
  \label{fig:typing-rules}
\end{figure}

Typing judgements are of the form $\ctx{\Gamma}{R} \vdash M : A$, which, when
the contexts are explicitly expanded, look like
$\ctxvar{x_1}{A_1}{\rho_1}, \ldots, \ctxvar{x_n}{A_n}{\rho_n} \vdash M : A$.
Each variable in the context carries a usage annotation, but the conclusion
is taken to always have annotation $1$.
Such a restriction is important to keep substitution admissible, as shown in
\cite{quantitative-type-theory}. \fixmeBA{probably drop the reference here (but keep it later on)}

A summary of the term syntax is given in \autoref{fig:terms}, and typing and
provisioning are defined together in \autoref{fig:typing-rules}.
As well as variables, and constructors and destructors for each type former, we
have a set of constants.
Constants are intended to give a way of producing and manipulating values of the
base type $\base$.

We write usage contexts $\resctx P$, $\resctx Q$, $\resctx R$ alone as
$\rescomment{\rho_1}x_1, \ldots, \rescomment{\rho_n}x_n$.
We understand a usage context as a row vector in a semimodule of which the
variables in scope form a basis.
As such, when writing a usage context, we may omit variables with annotation
$0$.
Using this convention, we write the $x$th basis vector as $1x$.
Such a context describes the restriction that semantically all variables except
$x$ are unused, while $x$ is used plainly.
This makes it the principle usage of the variable rule.

Constants $c$ can have arbitrary type $A_c$, but not arbitrary usage
restrictions.
Constants should have a global character, and not be able to depend on the
context in which they are used, so it makes sense to say that they are well
provisioned only in a context where all variables can be discarded. \fixmeBA{maybe an example?}

Among the other rules, notable features are the distinction between
\emph{tensor} ($\tensorOne$, $\tensor{}{}$) and \emph{with} ($\withTOne$,
$\withT{}{}$) products, and the presence of graded bangs ($\excl{\rho}{}$) and
lists.
A \emph{tensor} product is a pair in which both halves are present, so usages
add up, whereas a \emph{with} product is a pair for which the consumer can
choose which half they will use.
Lists are essentially \emph{tensor} products of their elements.
The graded bang encapsulates scaled usage of a value --- a value of type
$\excl{\rho}{A}$ is used by pattern matching, turning it into a variable of type
$A$ and annotation $\rho$.

Finally, we include a polymorphic type of lists.
Usage-wise, lists are a tensoring together of their elements, meaning that
$\nil$ requires its context to be discarded and $\cons{-}{-}$ requires the
context to be split between the head and the tail.
To eliminate a list, we will need the continuation handling
$\cons{-}{-}$ to be usable an arbitrary number of times.
The simplest way to achieve this irrespective of the usage algebra used is to
force all of the variables in the global context $\Gamma$ to have annotation
$0$.
For consistency, we put the same constraint on the continuation handling
$\nil{}$.
We believe that these restrictions could be loosened, allowing any deletable and
duplicable annotations to be on $\Gamma$, but have not worked out this variant.
% we split the context as in other pattern matching
% eliminators, but also require that both continuations use only discardable
% and duplicable variables.
% This ensures that, for example, the continuation that deals with $\cons{-}{-}$
% can be invoked as many times as necessary.

\fixmeBA{Could have separate $\resctx Q$ and $\resctx Q'$ for the nil and cons cases, and let the condition be $\subrctx{\resctx Q + \resctx Q'}{\resctx Q'}$?}

\subsection{Example programs}

\begin{example}
  To start, we assume that we have instantiated $\mathscr R$ to be
  $\{0,1,\omega\}$, with operations as defined in \autoref{sec:annotations}.
  We also assume a single base type $\base$, and no constants.
  In this case, the annotation $1$ corresponds to linear usage, and $\omega$
  corresponds to unrestricted usage.
  This means that there is no term of type
  $\ctxvar{x}{\base}{1} \vdash \tensor{\base}{\base}$, but we do have the
  following.

  \[
    \inferrule*[Right=$\tensor{}{}$-I]
    {
      \inferrule*[Right=var]
      {\rescomment{\omega \subres 1}}
      {\ctxvar{x}{\base}{\omega} \vdash x : \base}
      \\
      \inferrule*[Right=var]
      {\rescomment{\omega \subres 1}}
      {\ctxvar{x}{\base}{\omega} \vdash x : \base}
      \\ \rescomment{\omega \subres \omega + \omega}
    }
    {\ctxvar{x}{\base}{\omega} \vdash \ten{x}{x} : \tensor{\base}{\base}}
  \]

  We can abstract over this derivation by giving a closed term proving
  $\vdash \fun{\excl{\omega}{\base}}{\tensor{\base}{\base}}$.
  The new derivation extends the previous derivation by following the types.

  \[
    \inferrule*[Right=$\fun{}{}$-I]
    {
      \inferrule*[Right=$\excl{}{}$-E]
      {
        \inferrule*[Right=var]
        {\rescomment{1 \subres 1}}
        {\ctxvar{b_x}{\excl{\omega}{\base}}{1} \vdash b_x
          : \excl{\omega}{\base}}
        \\ \inferrule*{}
        {\nabla \\\\
          \ctxvar{b_x}{\excl{\omega}{\base}}{0}, \ctxvar{x}{\base}{\omega}
          \vdash \ten{x}{x} : \tensor{\base}{\base}}
        \\ \rescomment{1 \subres 1 + 0}
      }
      {\ctxvar{b_x}{\excl{\omega}{\base}}{1} \vdash \bm{}{b_x}{x}{\ten{x}{x}}
        : \tensor{\base}{\base}}
    }
    {\vdash \lam{b_x}{\bm{}{b_x}{x}{\ten{x}{x}}}
      : \fun{\excl{\omega}{\base}}{\tensor{\base}{\base}}}
  \]

  For $\nabla$, we have the previous derivation, but weakened by the addition of
  the assumption $\ctxvar{b_x}{\excl{\omega}{\base}}{0}$.
  In this case, it is easy to see that this weakening is permissible, because
  when it appears in the variable rule, we don't use $b_x$, so we need to show
  that it is discardable, which it is.
  In \lemref{lem:weak}, we will prove that such a weakening is generally
  admissible.

  That $b_x$ plays no role and places no restrictions on the continuation term
  suggests that we are justified to use pattern matching syntax colloquially.
  In this instance, we may write $\lam{\bang x}{M}$ to abbreviate
  $\lam{b_x}{\bm{b_x}{b_x}{x}{M}}$.
\end{example}

\fixmeBA{For each program should say why it is interesting: the three properties below because they correspond to the properties of a graded (lax-)monoidal comonoid comonad.}

\begin{example}[Monoidality]
We can show that $\excl{\rho}{}$ preserves tensor products in one direction:
$\fun{\tensor{\excl{\rho}{C}}{\excl{\rho}{D}}}{\excl{\rho}{(\tensor{C}{D})}}$.
Colloquially, we have the term
$\lam{\ten{\bang c}{\bang d}}{\bang{\ten{c}{d}}}$, which expands out to the
following formal term.

\[
  \begin{aligned}
    \lam{x_{bcbd}}{& \prm{ }{x_{bcbd}}{b_c}{b_d}{\\
        & \bm{ }{b_c}{c}{\\
          & \bm{ }{b_d}{d}{\\
            & \bang{\ten{c}{d}}}}}}
  \end{aligned}
\]

In the provisioning derivation, the interesting part is after all of the pattern
matching has been done and we are ready to start building the result.

\[
  \inferrule*[right=$\excl{\rho}{}$-I]{
    x_{bcbd}^0, b_c^0, b_d^0, c^\rho, d^\rho
    \subres \rho * x_{bcbd}^0, b_c^0, b_d^0, c^1, d^1
    \\
    \ctxvar{x_{bcbd}}{\tensor{\excl{\rho}{C}}{\excl{\rho}{D}}}{0},
    \ctxvar{b_c}{\excl{\rho}{C}}{0}, \ctxvar{b_d}{\excl{\rho}{D}}{0},
    \ctxvar{c}{C}{1}, \ctxvar{d}{D}{1}
    \vdash \ten{c}{d} : \tensor{C}{D}
  }
  {
    \ctxvar{x_{bcbd}}{\tensor{\excl{\rho}{C}}{\excl{\rho}{D}}}{0},
    \ctxvar{b_c}{\excl{\rho}{C}}{0}, \ctxvar{b_d}{\excl{\rho}{D}}{0},
    \ctxvar{c}{C}{\rho}, \ctxvar{d}{D}{\rho}
    \vdash \bang{\ten{c}{d}} : \excl{\rho}{(\tensor{C}{D})}
  }
\]

After this move, $c$ and $d$ become amenable to the var rule, which only applies
to super-usages of $1$.

The converse cannot be implemented in general because we get to a stage where we
are essentially trying to prove $\ctxvar{x_{cd}}{\tensor{C}{D}}{\rho} \vdash
\wn : \tensor{\excl{\rho}{C}}{\excl{\rho}{D}}$.
At this point, we cannot use the variable $x_{cd}$ because we might not have
$1 \subres \rho$, and so the condition of the var rule fails.
We can do $\otimes$-I, but there is no satisfactory way to split $x_{cd}^\rho$
between the two halves.
The interpretation of this non-derivability in linear logic is that
$\excl{\omega}{(\tensor{C}{D})}$ promises an unlimited supply of $C$s and $D$s
\emph{in lock step}, whereas $\tensor{\excl{\omega}{C}}{\excl{\omega}{C}}$
promises both an unlimited supply of $C$s and an unlimited supply of $D$s with
no relation between each other.
\end{example}

\begin{example}[Comonad operations]
The following two terms are well typed and well provisioned.

\begin{displaymath}
  \begin{eqns}
    \lam{\bang a}{a} &:& \fun{\excl{1}{A}}{A} \\
    \lam{\bang a}{\bang{\bang{a}}}
    &:& \fun{\excl{\pi * \rho}{A}}{\excl{\pi}{\excl{\rho}{A}}}
  \end{eqns}
\end{displaymath}
%We have the following two derivations.
%
%\begin{mathpar}
%  \inferrule*[Right=$\multimap$-I]{
%    \inferrule*[Right=$\excl{1}{}$-E]{
%      \inferrule*[right=var]{ }
%      {\ctxvar{ba}{\excl{1}{A}}{1} \vdash ba : \excl{1}{A}}
%      \\
%      \inferrule*[Right=var]{ }
%      {\ctxvar{ba}{\excl{1}{A}}{0}, \ctxvar{a}{A}{1} \vdash a : A}
%    }
%    {\ctxvar{ba}{\excl{1}{A}}{1} \vdash \bm{A}{ba}{a}{a} : A}
%  }
%  {\vdash \lam{ba}{\bm{A}{ba}{a}{a}} : \fun{\excl{1}{A}}{A}}
%
%  \and
%
%  \inferrule*[Right=$\multimap$-I]{
%    \inferrule*[Right=$\excl{\pi * \rho}{}$-E]{
%      \inferrule*[right=var]{ }
%      {\ctxvar{ba}{\excl{\pi * \rho}{A}}{1} \vdash ba : \excl{\pi * \rho}}
%      \\
%      \inferrule*[Right=$\excl{\pi}{}$-I]{
%        \inferrule*[Right=$\excl{\rho}{}$-I]{
%          \inferrule*[Right=var]{ }
%          {\ctxvar{ba}{\excl{\pi * \rho}{A}}{0}, \ctxvar{a}{A}{1} \vdash
%            a : A}
%        }
%        {\ctxvar{ba}{\excl{\pi * \rho}{A}}{0}, \ctxvar{a}{A}{\rho} \vdash
%          \bang{a} : \excl{\rho}{A}}
%      }
%      {\ctxvar{ba}{\excl{\pi * \rho}{A}}{0}, \ctxvar{a}{A}{\pi * \rho} \vdash
%        \bang{\bang{a}} : \excl{\pi}{\excl{\rho}{A}}}
%    }
%    {\ctxvar{ba}{\excl{\pi * \rho}{A}}{1} \vdash
%      \bm{A}{ba}{a}{\bang{\bang{a}}} : \excl{\pi}{\excl{\rho}{A}}}
%  }
%  {\vdash \lam{ba}{\bm{A}{ba}{a}{\bang{\bang{a}}}}
%    : \fun{\excl{\pi * \rho}{A}}{\excl{\pi}{\excl{\rho}{A}}}}
%\end{mathpar}

These are the counit and comultiplication required in showing that $\excl{(-)}{}$ is a
graded comonad in our category of types and functions.
\end{example}

\fixme{exponentials turn additives into multiplicatives}

\fixmeBA{Perhaps say what happens in ``extreme''/''interesting'' cases: e.g. $\mathcal{R} = 1$, and the paragraph from next subsection about collapsing of tensor and with}

\subsection{Admissible rules}
\label{sec:admissible}

\begin{figure}
  \begin{mathpar}
    \inferrule*[right=subuse]{
      \ctx{\Gamma}{Q} \vdash M : A
      \\ \resctx P \subres \resctx Q
    }
    {
      \ctx{\Gamma}{P} \vdash M : A
    }
    \and
    \inferrule*[right=weak]{
      \ctx{\Gamma}{P} \vdash M : A
      \\ \resctx Q \subres \vct 0
    }
    {
      \ctx{\Gamma}{P}, \ctx{\Delta}{Q} \vdash M : A
    }
    \and
    \inferrule*[right=subst]{
      \ctx{\Delta}{Q} \vdash N : A
      \\ \ctx{\Gamma}{P} \stackrel\sigma\Rightarrow \ctx{\Delta}{Q}
    }
    {
      \ctx{\Gamma}{P} \vdash N[\sigma] : A
    }
  \end{mathpar}
  \caption{Admissible rules}
  \label{fig:admissible-rules}
\end{figure}

\fixmeBA{An introductory sentence to summarise this section.}

Recalling from \autoref{sec:annotations}, $\rescomment \pi \subres \rescomment
\rho$ says that $\rescomment \rho$ places more relaxed restrictions than
$\rescomment \pi$.
When lifted pointwise, $\resctx P \subres \resctx Q$ means that
$\resctx Q$ is a more liberal context than $\resctx
 P$, so any term well provisioned in $\resctx P$ is also well
provisioned in $\resctx Q$.

\begin{lemma}[sub-usaging]
  \autoref{fig:admissible-rules}: \TirName{subuse}
\end{lemma}
\begin{proof}
  By induction on the provisioning derivation until a rule with splitting is
  encountered.
  Here, we will just consider splitting by addition, but the other cases are
  similar.
  Suppose we are given a derivation of $\ctx{\Gamma}{P} \vdash M : A$,
  from which one of the premises is
  $\resctx P_0 + \resctx P_1 \subres \resctx P$.
  Then, because $\resctx P \subres \resctx Q$, we also have
  $\resctx P_0 + \resctx P_1 \subres \resctx Q$.
  Therefore, we can give a derivation for
  $\ctx{\Gamma}{Q} \vdash M : A$ which is the same as the derivation we
  were given, except with the updated splitting premise.
\end{proof}

If addition is the meet of the sub-usaging order, then the distinction between
tensor and with products collapses.
This fact can be seen clearly in the introduction rules, where the condition
$\resctx R \subres \resctx P + \resctx Q$ is equivalent to the conjunction of
$\resctx R \subres \resctx P$ and $\resctx R \subres \resctx Q$.
With these and the admissibility of sub-usaging, we get the equivalence.
\fixmeBA{Can this be turned into a proposition? E.g., there are terms translating back and forth between both types of product? This observation might make more sense in the previous subsection. Also, it extends to the units too?}

Famously, linear logic does not allow unrestricted weakening.
Our system retains this in spirit, but contains a restricted variant.
Thinking of the $\{0,1,\omega\}$ instantiation, the following rule says that
\emph{absent} and \emph{unrestricted} variables may be weakened, but linear
variables may not be.
In the monotonicity instantiation, all annotations are above $0$, so weakening
can be applied to any variable.

\begin{lemma}[Weakening]
  \label{lem:weak}
  We can add unmentioned free variables if those variables are annotated with a
  weakenable annotation.
  \autoref{fig:admissible-rules}: \TirName{weak}
\end{lemma}

When $0$ is the top element of the subusaging order, the condition
$\resctx Q \subres \vct 0$ is trivial, so arbitrary weakening is allowed.

Our main syntactic lemma is substitution.
To state and prove substitution, the key insight is that semirings give rise to
an important fragment of linear algebra.
In particular, we can form vectors and matrices of semiring elements, and define
addition, scaling, and multiplication.
Addition and scaling were the two operators used in premises of typing rules.
Multiplication will be used to apply a substitution to a context.

\fixme{\ldots example of why na\"{i}ve substitution doesn't work}

Let $|-|$ denote the number of free variables of a given context.
In the following definition, usage contexts ($\resctx P$ and $\resctx Q$) and
basis vectors are understood to be row vectors, and juxtaposition of vectors and
matrices denotes multiplication.

\begin{definition}[Well provisioned substitution]
  A substitution $\sigma$ from $\Gamma^{\resctx P}$ to
  $\Delta^{\resctx Q}$ is a tuple with the following data.

  \begin{itemize}
  \item a $|\resctx Q| \times |\resctx P|$ matrix of usage annotations $\mat
    \Sigma$ such that $\resctx P \subres \resctx Q \mat \Sigma$
  \item for each $(x:A) \in \Delta$, a term $M_x$ such that
    $\Gamma^{\vct e_x \mat \Sigma} \vdash M_x : A$
  \end{itemize}
\end{definition}

The idea is that each term $M_x$ is well provisioned at row $x$ of $\mat \Sigma$.
The element $\resctx Q_x$ tells us how many ``copies'' of $M_x$ we need, so the
requirements for the whole simultaneous substitution are a sum of the rows of
$\mat \Sigma$ weighted by $\resctx Q$.

\begin{lemma}[Substitution]
  \autoref{fig:admissible-rules}: \TirName{subst}
\end{lemma}
\renewcommand{\proofname}{Proof sketch}
\begin{proof}
  By induction on the provisioning derivation.
  Wherever a context is split, by any of $\resctx R \subres \vct 0$,
  $\resctx R \subres \resctx P + \resctx Q$, or
  $\resctx R \subres \rescomment \rho * \resctx P$, the linearity of vector-matrix
  multiplication means that $\mat \Sigma$ can stay the same.
  When going under a binder, the new substitution matrix will be constructed
  from the given $\mat \Sigma$ as the following.
  \newcommand*{\mathhuge}[1]{\mathlarger{\mathlarger{\mathlarger{\mathlarger{#1}}}}}
  \[
    \operatorname{extend}~\mat \Sigma \coloneqq
    %\left( \begin{array}{cccc}
    %  & & & 0 \\
    %  & \mathhuge{\mat \Sigma} & & \vdots \\
    %  & & & 0 \\
    %  0 & \cdots & 0 & 1
    %\end{array} \right)
    \begin{pmatrix}
      \mat \Sigma_{1,1} & \cdots & \mat \Sigma_{1,n} & 0 \\
      \vdots & \ddots & \vdots & \vdots \\
      \mat \Sigma_{m,1} & \cdots & \mat \Sigma_{m,n} & 0 \\
      0 & \cdots & 0 & 1
    \end{pmatrix}
    %\begin{pmatrix}
    %  \mathhuge{\mat \Sigma} & \begin{array}{c} 0 \\ \vdots \\ 0 \end{array} \\
    %  \begin{array}{ccc} 0 & \cdots & 0 \end{array} & 1
    %\end{pmatrix}
    %\begin{pmatrix}
    %  \multicolumn{3}{c}{\multirow{3}{*}{A}} & 0 \\
    %  \multicolumn{3}{c}{} & \vdots \\
    %  \multicolumn{3}{c}{} & 0 \\
    %  0 & \cdots & 0 & 1
    %\end{pmatrix}
  \]
  The new column says that to substitute the new variable by itself once, we
  need $1$ use of it, and $0$ use of any other variable.
  The new row says that the new variable will \emph{only} be used as a
  substitute for itself, and nowhere else.
\end{proof}
\renewcommand{\proofname}{Proof}

\subsection{Equivalence to DILL}
\label{sec:dill}

\fixmeBA{Would better say ``Translating from DILL to \name{} and
  back'': we don't really say in what sense we mean ``equivalent'';
  Barber's thesis
  (\url{https://www.era.lib.ed.ac.uk/bitstream/handle/1842/392/ECS-LFCS-97-371.pdf},
  page 51) proves a round trip property using the equational theories
  of the respective calculi, but we haven't written out an equational
  theory for \name{} yet}

\fixmeBA{Should say why we are interested in such a translation}

Dual Intuitionistic Linear Logic is a particular formulation of intuitionistic
linear logic described in \cite{Barber1996}.
Its key feature, which simplifies the metatheory of linear logic, is the use of
separate contexts for linear and intuitionistic free variables.
Here we show that DILL is a fragment of the instantiation of \name{} at the
linearity semiring $\{0,1,\omega\}$.\fixmeBA{also the fragment without lists}

The types of DILL are the same as the types of \name, except for the
restriction of $\excl{\rho}{}$ to just $\excl{\omega}{}$.
We will write the latter simply as $\oc$ when it appears in DILL.
We add sums and with products to the calculus of \cite{Barber1996}, with the
obvious rules.\fixmeBA{should put the rules for the precise variant of DILL we are using in an appendix}
These additive type formers present no additional difficulty to the translation.

\fixmeBA{Should give the translation on terms}

%\begin{figure}
%  \begin{displaymath}
%    \begin{eqns}
%      \mathrm{DILL} &\hookrightarrow& \name \\
%      Y &\mapsto& \base_Y \\
%      I &\mapsto& \tensorOne \\
%      A \multimap B &\mapsto& \fun{A}{B} \\
%      A \otimes B &\mapsto& \tensor{A}{B} \\
%      \oc A &\mapsto& \excl{\omega}{A}
%    \end{eqns}
%  \end{displaymath}
%  \label{fig:dill}
%  \caption{Embedding of DILL types into \name}
%\end{figure}

\begin{proposition}[DILL $\to$ \name]
  Given a DILL derivation of \[
    \Gamma; \Delta \vdash t : A,
  \] we can produce a $\name_{01\omega}$ derivation of \[
    \ctx{\Gamma}{\vct \omega}, \ctx{\Delta}{\vct 1} \vdash
    M_t : A
  \] for some term $M_t$.
\end{proposition}
\begin{proof}
  By induction on the derivation.
  We have $\omega \subres 0$, which allows us to discard intuitionistic
  variables at the var rules, and both $1 \subres 1$ and $\omega \subres 1$,
  which allow us to use both linear and intuitionistic variables.

  Weakening is used when splitting linear variables between two premises.
  For example, \TirName{$\otimes$-I} is as follows.
  \[
    \inferrule*[right=$\otimes$-I]
    {\Gamma; \Delta_t \vdash t : A \\ \Gamma; \Delta_u \vdash u : B}
    {\Gamma; \Delta_t, \Delta_u \vdash t \otimes u : A \otimes B}
  \]
  From this, our new derivation is as follows.
  \[
    \inferrule*[right=$\tensor{}{}$-I]
    {
      \inferrule*[right=weak]
      {\mathit{ih}_t \\\\
        \ctx{\Gamma}{\vct \omega}, \ctx{\Delta_t}{\vct 1} \vdash M_t : A}
      {\ctx{\Gamma}{\vct \omega}, \ctx{\Delta_t}{\vct 1}, \ctx{\Delta_u}{\vct 0}
        \vdash M_t : A}
      \\
      \inferrule*[right=weak]
      {\mathit{ih}_u \\\\
        \ctx{\Gamma}{\vct \omega}, \ctx{\Delta_u}{\vct 1} \vdash M_u : A}
      {\ctx{\Gamma}{\vct \omega}, \ctx{\Delta_t}{\vct 0}, \ctx{\Delta_u}{\vct 1}
        \vdash M_u : A}
    }
    {\ctx{\Gamma}{\vct \omega}, \ctx{\Delta_t}{\vct 1}, \ctx{\Delta_u}{\vct 1}
      \vdash \ten{M_t}{M_u} : \tensor{A}{B}}
  \]
\end{proof}

\begin{proposition}[\name $\to$ DILL]
  Given a $\name_{01\omega}$ derivation of \[
    \ctx{\Gamma}{\vct \omega}, \ctx{\Delta}{\vct 1}, \ctx{\Theta}{\vct 0} \vdash
    M : A,
  \] which contains only types expressible in DILL, we can produce a DILL
  derivation of \[
    \Gamma; \Delta \vdash t_M : A
  \] for some term $t_M$.
\end{proposition}
\begin{proof}
  By induction on the derivation.

  In the case of \TirName{var}, all of the unused variables have annotation
  greater than $0$, i.e., $0$ or $\omega$.
  Those annotated $0$ are absent from the DILL derivation, and those annotated
  $\omega$ are in the intuitionistic context.
  The used variable is annotated either $1$ or $\omega$.
  In the first case, we use \TirName{Lin-Ax}, and in the second case,
  \TirName{Int-Ax}.

  All binding of variables in \name{} maps directly onto DILL.

  For the \TirName{$\excl{\omega}{}$-I} rule, we are required to prove that the
  following is admissible in DILL.
  \[
    \inferrule*
    {\Gamma; \Delta \vdash t_M : A}
    {\Gamma, \Delta; \cdot \vdash \oc t_M : \oc A}
  \]
  This follows from the Environment Weakening property from Lemma 3.1 of
  \cite{Barber1996} applied to $\Delta$, followed by \TirName{$\oc$-I}.

  Splitting of contexts is more involved.
  We shall again take \TirName{$\tensor{}{}$-I} as our example.
  We are trying to prove admissible \[
    \inferrule*
    {\Gamma_A; \Delta_A \vdash t_M : A \\ \Gamma_B; \Delta_B \vdash t_N}
    {\Gamma; \Delta \vdash t_M \otimes t_N : A \otimes B}
  \] given also that $\Delta$ consists of only (but not necessarily all)
  variables that occur exactly once in $\Delta_A, \Delta_B$ and not at all in
  $\Gamma_A, \Gamma_B$, and that $\Gamma$ consists of all of the other variables
  from the premises.
  To prove this rule admissible, on both premises we apply Environment Weakening
  to all variables in $\Delta_A$ or $\Delta_B$ that are also in $\Gamma$.
  This gives us $\Gamma; \Delta_A' \vdash t_M : A$ and $\Gamma; \Delta_B' \vdash
  t_N : B$, with $\Delta_A', \Delta_B' = \Delta$.
  At this point, we can apply \TirName{$\otimes$-I} and be done.
\end{proof}